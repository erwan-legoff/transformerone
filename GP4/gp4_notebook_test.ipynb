{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from FileUtils import *  # Assuming this module remains unchanged\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_dict_to_file(data, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        for key, value in data.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "def save_list_to_file(data, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(f\"{item}\\n\")\n",
    "\n",
    "def save_str_to_file(data, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        f.write(data)\n",
    "\n",
    "\n",
    "def write_first_2000_chars_to_file(text, file_name):\n",
    "    save_str_to_file(text[:2000], file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Unicode normalization functions ---\n",
    "def to_decomposed_unicode(text: str) -> str:\n",
    "    return unicodedata.normalize('NFD', text)\n",
    "\n",
    "def to_unified_unicode(text: str) -> str:\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "# --- Tokenization/detokenization functions ---\n",
    "def tokenize(text, string_to_int, max_gram_chars= 8):\n",
    "    text = to_decomposed_unicode(text)\n",
    "    unknown_token = 0 \n",
    "    int_tokens = []\n",
    "    c = 0\n",
    "    while c < len(text):\n",
    "        remaining_chars = len(text) - c\n",
    "        current_char_size = min(max_gram_chars, remaining_chars) \n",
    "        not_found = True\n",
    "        while not_found and current_char_size > 0:\n",
    "            current_n_gram = text[c:c+current_char_size]\n",
    "            if(current_n_gram in string_to_int):\n",
    "                int_tokens.append(string_to_int[current_n_gram]) \n",
    "                not_found = False\n",
    "            else:\n",
    "                current_char_size-=1\n",
    "            \n",
    "        if not_found:\n",
    "            token = string_to_int.get(text[c], unknown_token)\n",
    "            int_tokens.append(token)\n",
    "            c += 1\n",
    "        else:\n",
    "            c += current_char_size\n",
    "    return int_tokens\n",
    "\n",
    "def detokenize(int_tokens, int_to_string):\n",
    "    return to_unified_unicode(''.join([int_to_string[i] for i in int_tokens]))\n",
    "\n",
    "# --- Data loading ---\n",
    "def load_data(training_file, evaluation_file):\n",
    "    with open(training_file, 'r', encoding='utf-8') as f:\n",
    "        training_text = f.read()\n",
    "    with open(evaluation_file, 'r', encoding='utf-8') as f:\n",
    "        eval_text = f.read()\n",
    "    return training_text, eval_text\n",
    "\n",
    "# --- Occurrence counting ---\n",
    "def count_bigram_occurences(training_text):\n",
    "    bigram_occurences = {}\n",
    "    for c in range(len(training_text)-1):\n",
    "        bigram = training_text[c] + training_text[c+1]\n",
    "        bigram_occurences[bigram] = bigram_occurences.get(bigram, 0) + 1\n",
    "    return bigram_occurences\n",
    "\n",
    "def count_n_gram_occurences(training_data, gram_size):\n",
    "    bigram_occurences = {}\n",
    "    for i in range(len(training_data) - gram_size + 1):\n",
    "        sub = training_data[i : i + gram_size]\n",
    "        \n",
    "        # Si training_data est une liste, sub = [x, y, ...] => non hashable\n",
    "        # Donc on le convertit en tuple\n",
    "        if isinstance(training_data, list):\n",
    "            sub = tuple(sub)\n",
    "        \n",
    "        bigram_occurences[sub] = bigram_occurences.get(sub, 0) + 1\n",
    "    return bigram_occurences\n",
    "import random\n",
    "def count_n_gram_occurences_optimized(training_data, gram_size, max_char_skip = 10):\n",
    "    bigram_occurences = {}\n",
    "    c = 0\n",
    "    while c < len(training_data) - gram_size + 1:\n",
    "        sub = training_data[c : c + gram_size]\n",
    "        \n",
    "        # Si training_data est une liste, sub = [x, y, ...] => non hashable\n",
    "        # Donc on le convertit en tuple\n",
    "        if isinstance(training_data, list):\n",
    "            sub = tuple(sub)\n",
    "        \n",
    "        bigram_occurences[sub] = bigram_occurences.get(sub, 0) + 1\n",
    "        current_max_c = (len(training_data) - gram_size)\n",
    "        remaining_chars = current_max_c - c\n",
    "        current_max_skip = min(max_char_skip,remaining_chars)\n",
    "        # We want to skip randomly to echantillonized our data\n",
    "        next_offset = random.randint(1,1+current_max_skip)\n",
    "        c += next_offset\n",
    "    return bigram_occurences\n",
    "import re\n",
    "import re\n",
    "import random\n",
    "\n",
    "def count_n_gram_occurences_optimized_no_ponctuation(training_data, gram_size, int_to_string, max_char_skip=10):\n",
    "    n_gram_occurrences = {}\n",
    "    c = 0\n",
    "    # Compilation de la regex pour éviter de la recompiler à chaque itération\n",
    "    pattern = re.compile(r\"[.,;:!?'\\\"()«»—\\-]\")\n",
    "    data_len = len(training_data)\n",
    "    limit = data_len - gram_size + 1\n",
    "\n",
    "    while c < limit:\n",
    "        sub = training_data[c : c + gram_size]\n",
    "        # Si training_data est une liste, convertissons le sous-ensemble en tuple (hashable)\n",
    "        if isinstance(training_data, list):\n",
    "            sub = tuple(sub)\n",
    "\n",
    "        # Vérifie si au moins un caractère de 'sub' correspond à une ponctuation.\n",
    "        # Utilisation d'une expression génératrice et pattern.search pour un test court-circuité.\n",
    "        if not any(pattern.search(int_to_string.get(item, \"\")) for item in sub):\n",
    "            n_gram_occurrences[sub] = n_gram_occurrences.get(sub, 0) + 1\n",
    "\n",
    "        # Calcul du nombre de caractères restants pour définir le saut aléatoire\n",
    "        remaining = data_len - (c + gram_size)\n",
    "        current_max_skip = min(max_char_skip, remaining)\n",
    "        next_offset = random.randint(1, 1 + current_max_skip)\n",
    "        c += next_offset\n",
    "\n",
    "    return n_gram_occurrences\n",
    "\n",
    "\n",
    "def count_char_occurences(training_text):\n",
    "    char_occurences = {}\n",
    "    for c in training_text:\n",
    "        char_occurences[c] = char_occurences.get(c, 0) + 1\n",
    "    return char_occurences\n",
    "\n",
    "def get_top_n_grams(n_gram_occurences, max_size):\n",
    "    \"\"\"\n",
    "    Sorts a dictionary of n-gram occurrences and keeps the `max_size` most frequent ones.\n",
    "    \n",
    "    :param n_gram_occurences: Dictionary containing n-grams and their occurrences.\n",
    "    :param max_size: Maximum number of elements to keep.\n",
    "    :return: Sorted dictionary with the most frequent n-grams.\n",
    "    \"\"\"\n",
    "    return dict(sorted(n_gram_occurences.items(), key=lambda item: item[1], reverse=True)[:max_size])\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def save_sorted_n_gram_occurences(n_gram_occurences, file_name, directory=\"vocabulary\"):\n",
    "    \"\"\"\n",
    "    Sorts n-gram occurrences by descending frequency and saves them to a file.\n",
    "\n",
    "    :param n_gram_occurences: Dictionary containing n-grams and their occurrences.\n",
    "    :param file_name: Output file name.\n",
    "    :param directory: Directory to store the files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    sorted_n_grams = sorted(n_gram_occurences.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    # Save sorted occurrences\n",
    "    with open(os.path.join(directory, file_name), 'w', encoding='utf-8') as f:\n",
    "        for n_gram, count in sorted_n_grams:\n",
    "            f.write(f\"{n_gram}\\t{count}\\n\")\n",
    "    \n",
    "    return sorted_n_grams  # Return the sorted list for later use\n",
    "\n",
    "def merge_in_place(token_sequence, bigram, new_token):\n",
    "    \"\"\"\n",
    "    Fusionne le bigram 'bigram' par 'new_token' dans 'token_sequence'.\n",
    "    Renvoie une NOUVELLE liste de tokens après la fusion.\n",
    "    \n",
    "    :param token_sequence: liste d’entiers (IDs de tokens).\n",
    "    :param bigram: tuple (token_id_1, token_id_2) à fusionner.\n",
    "    :param new_token: entier représentant l'ID du nouveau token.\n",
    "    :return: nouvelle liste de tokens où les occurrences de bigram sont remplacées par new_token.\n",
    "    \"\"\"\n",
    "    \n",
    "    i,j = 0,0\n",
    "    token_count = len(token_sequence)\n",
    "    merged_sequence = [None] * token_count\n",
    "    b0, b1 = bigram\n",
    "    while i < token_count:\n",
    "        # Si on est sur l'avant-dernier token, on peut regarder la paire (i, i+1)\n",
    "        if i < token_count - 1 and token_sequence[i] == b0 and token_sequence[i+1] == b1:\n",
    "            # On remplace la paire par le nouveau token\n",
    "            merged_sequence[j]=new_token\n",
    "            j += 1\n",
    "\n",
    "            i += 2\n",
    "        else:\n",
    "            # Sinon on recopie le token courant tel quel\n",
    "            merged_sequence[j]=token_sequence[i]\n",
    "            j += 1\n",
    "            i += 1\n",
    "\n",
    "    return merged_sequence[:j]\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    with open(tokenizer_path, 'r', encoding='utf-8') as f:\n",
    "        tokenizer_data = json.load(f)\n",
    "    \n",
    "    string_to_int = tokenizer_data[\"string_to_int\"]\n",
    "    int_to_string = tokenizer_data[\"int_to_string\"]\n",
    "    \n",
    "    print(f\"Tokenizer loaded: {tokenizer_path}\")\n",
    "    return string_to_int, int_to_string\n",
    "\n",
    "def save_tokenizer(string_to_int, int_to_string, tokenization_iteration, max_char_skip, directory=\"tokenizers\"):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    # Construire une chaîne représentant les paramètres du tokenizer\n",
    "    tp_str = f\"iter{tokenization_iteration}_skip{max_char_skip}\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%Hh\")\n",
    "    tokenizer_name = f\"tokenizer_{tp_str}_{timestamp}.json\"\n",
    "    tokenizer_path = os.path.join(directory, tokenizer_name)\n",
    "    \n",
    "    tokenizer_data = {\n",
    "        \"string_to_int\": string_to_int,\n",
    "        \"int_to_string\": int_to_string\n",
    "    }\n",
    "    \n",
    "    with open(tokenizer_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(tokenizer_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Tokenizer saved: {tokenizer_path}\")\n",
    "    return tokenizer_path\n",
    "\n",
    "# --- Vocabulary creation ---\n",
    "def create_vocabularies_V2(training_text, \n",
    "                        max_bigrams=538, \n",
    "                        max_trigrams=1000, max_quadgrams=1500, \n",
    "                        max_pentagrams=2173, max_sextegrams=7000,\n",
    "                        max_septegrams = 7000, max_octograms=7000, \n",
    "                        directory=\"vocabulary_v2\",\n",
    "                        tokenization_iteration = 1000,\n",
    "                        max_char_skip = 50):\n",
    "    char_occurences = count_char_occurences(training_text)\n",
    "    sorted_chars = sorted(char_occurences.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_chars = dict(sorted_chars)\n",
    "    current_vocabulary = list(top_chars.keys())\n",
    "    # Juste après avoir construit top_chars et current_vocabulary:\n",
    "    all_chars_in_text = set(training_text)  # l'ensemble de tous les caractères distincts\n",
    "    dict_chars = set(current_vocabulary)    # l'ensemble des chars que vous avez retenus\n",
    "\n",
    "    missing_chars = all_chars_in_text - dict_chars\n",
    "    if len(missing_chars) > 0:\n",
    "        print(\"Caractères manquants (non couverts par le vocabulaire) :\", missing_chars)\n",
    "    else:\n",
    "        print(\"Tous les caractères du texte sont couverts par le vocabulaire initial.\")\n",
    "        current_string_to_int = {string: idx for idx, string in enumerate(current_vocabulary)}\n",
    "        current_int_to_string = {idx: string for idx, string in enumerate(current_vocabulary)}\n",
    "    \n",
    "    tokenized_compressed_text = tokenize(training_text,current_string_to_int,max_gram_chars=1)\n",
    "    # 1. **Count char occurrences**\n",
    "    for i in range(tokenization_iteration):\n",
    "        print(\"bigram_occurences_start\")\n",
    "        bigram_occurences = count_n_gram_occurences_optimized_no_ponctuation(tokenized_compressed_text, gram_size=2, int_to_string=current_int_to_string,max_char_skip=max_char_skip,)\n",
    "        print(\"bigram_occurences_end\")\n",
    "\n",
    "        sorted_bigrams = sorted(bigram_occurences.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        \n",
    "\n",
    "        # 3. **Select the top N most frequent**\n",
    "        if not sorted_bigrams:\n",
    "            print(\"Aucun bigram trouvé, fin du processus.\")\n",
    "            break  # Stopper si plus de bigrams à fusionner\n",
    "        \n",
    "        current_top_bigram_ints = sorted_bigrams[0][0]\n",
    "        current_top_bigram_strings = current_int_to_string.get(current_top_bigram_ints[0], \"\") + \\\n",
    "                                     current_int_to_string.get(current_top_bigram_ints[1], \"\")\n",
    "\n",
    "\n",
    "        current_vocabulary.append(current_top_bigram_strings)\n",
    "\n",
    "        \n",
    "        new_token_id = len(current_string_to_int)\n",
    "        current_string_to_int[current_top_bigram_strings] = new_token_id\n",
    "        current_int_to_string[new_token_id] = current_top_bigram_strings\n",
    "        print('merge_in_place_start')\n",
    "        print(i)\n",
    "        tokenized_compressed_text  = merge_in_place(tokenized_compressed_text, current_top_bigram_ints,new_token_id)\n",
    "        print('merge_in_place_end')\n",
    "\n",
    "\n",
    "\n",
    "    # 5. **Create the combined vocabulary**\n",
    "    full_vocabulary = current_vocabulary\n",
    "        \n",
    "    # 6. **Save the full vocabulary**\n",
    "    save_list_to_file(full_vocabulary, os.path.join(directory, 'full_vocabulary.txt'))\n",
    "\n",
    "    # 7. **Create mappings for tokenization**\n",
    "    vocabulary_size = len(full_vocabulary)\n",
    "    string_to_int = {string: idx for idx, string in enumerate(full_vocabulary)}\n",
    "    int_to_string = {idx: string for idx, string in enumerate(full_vocabulary)}\n",
    "    tokenizer_path = save_tokenizer(string_to_int, int_to_string, tokenization_iteration, max_char_skip)\n",
    "    return vocabulary_size, string_to_int, int_to_string, tokenizer_path\n",
    "\n",
    "def create_vocabularies(training_text, \n",
    "                        max_bigrams=538, max_chars=117, \n",
    "                        max_trigrams=1000, max_quadgrams=1500, \n",
    "                        max_pentagrams=2173, max_sextegrams=7000,\n",
    "                        max_septegrams = 7000, max_octograms=7000, \n",
    "                        directory=\"vocabulary\"):\n",
    "\n",
    "    # 1. **Count occurrences**\n",
    "    octogram_occurences = count_n_gram_occurences(training_text, gram_size=8)\n",
    "    septegram_occurences = count_n_gram_occurences(training_text, gram_size=7)\n",
    "    sextegram_occurences = count_n_gram_occurences(training_text, gram_size=6)\n",
    "    pentagram_occurences = count_n_gram_occurences(training_text, gram_size=5)\n",
    "    quadgram_occurences = count_n_gram_occurences(training_text, gram_size=4)\n",
    "    trigram_occurences = count_n_gram_occurences(training_text, gram_size=3)\n",
    "    bigram_occurences = count_n_gram_occurences(training_text, gram_size=2)\n",
    "    char_occurences = count_char_occurences(training_text)\n",
    "\n",
    "    # 2. **Sort and save occurrences in \"vocabulary/\"**\n",
    "    sorted_octograms = save_sorted_n_gram_occurences(octogram_occurences, 'octogram_occurences.txt', directory)\n",
    "    sorted_septegrams = save_sorted_n_gram_occurences(septegram_occurences, 'septegram_occurences.txt', directory)\n",
    "    sorted_sextegrams = save_sorted_n_gram_occurences(sextegram_occurences, 'sextegram_occurences.txt', directory)\n",
    "    sorted_pentagrams = save_sorted_n_gram_occurences(pentagram_occurences, 'pentagram_occurences.txt', directory)\n",
    "    sorted_quadgrams = save_sorted_n_gram_occurences(quadgram_occurences, 'quadgram_occurences.txt', directory)\n",
    "    sorted_trigrams = save_sorted_n_gram_occurences(trigram_occurences, 'trigram_occurences.txt', directory)\n",
    "    sorted_bigrams = save_sorted_n_gram_occurences(bigram_occurences, 'bigram_occurences.txt', directory)\n",
    "    sorted_chars = save_sorted_n_gram_occurences(char_occurences, 'char_occurences.txt', directory)\n",
    "\n",
    "    # 3. **Select the top N most frequent**\n",
    "    top_octograms = dict(sorted_octograms[:max_octograms])\n",
    "    top_septegrams = dict(sorted_septegrams[:max_septegrams])\n",
    "    top_sextegrams = dict(sorted_sextegrams[:max_sextegrams])\n",
    "    top_pentagrams = dict(sorted_pentagrams[:max_pentagrams])\n",
    "    top_quadgrams = dict(sorted_quadgrams[:max_quadgrams])\n",
    "    top_trigrams = dict(sorted_trigrams[:max_trigrams])\n",
    "    top_bigrams = dict(sorted_bigrams[:max_bigrams])\n",
    "    top_chars = dict(sorted_chars[:max_chars])\n",
    "\n",
    "    # 4. **Save the truncated N-grams**\n",
    "    save_dict_to_file(top_octograms, os.path.join(directory, 'top_octograms.txt'))\n",
    "    save_dict_to_file(top_septegrams, os.path.join(directory, 'top_septegrams.txt'))\n",
    "    save_dict_to_file(top_sextegrams, os.path.join(directory, 'top_sextegrams.txt'))\n",
    "    save_dict_to_file(top_pentagrams, os.path.join(directory, 'top_pentagrams.txt'))\n",
    "    save_dict_to_file(top_quadgrams, os.path.join(directory, 'top_quadgrams.txt'))\n",
    "    save_dict_to_file(top_trigrams, os.path.join(directory, 'top_trigrams.txt'))\n",
    "    save_dict_to_file(top_bigrams, os.path.join(directory, 'top_bigrams.txt'))\n",
    "    save_dict_to_file(top_chars, os.path.join(directory, 'top_chars.txt'))\n",
    "\n",
    "    # 5. **Create the combined vocabulary**\n",
    "    full_vocabulary = (\n",
    "        list(top_octograms.keys()) +\n",
    "        list(top_septegrams.keys()) + \n",
    "        list(top_sextegrams.keys()) + \n",
    "        list(top_pentagrams.keys()) + \n",
    "        list(top_quadgrams.keys()) + \n",
    "        list(top_trigrams.keys()) + \n",
    "        list(top_bigrams.keys()) + \n",
    "        list(top_chars.keys())\n",
    "    )\n",
    "\n",
    "    # 6. **Save the full vocabulary**\n",
    "    save_list_to_file(full_vocabulary, os.path.join(directory, 'full_vocabulary.txt'))\n",
    "\n",
    "    # 7. **Create mappings for tokenization**\n",
    "    vocabulary_size = len(full_vocabulary)\n",
    "    string_to_int = {string: idx for idx, string in enumerate(full_vocabulary)}\n",
    "    int_to_string = {idx: string for idx, string in enumerate(full_vocabulary)}\n",
    "\n",
    "    return vocabulary_size, string_to_int, int_to_string\n",
    "\n",
    "\n",
    "# --- Prepare tokenized data tensors ---\n",
    "def prepare_tokenized_data(training_text, eval_text, tokenize_func, string_to_int, max_train_tokens=None):\n",
    "    tokenized_training_data = torch.tensor(tokenize_func(training_text, string_to_int), dtype=torch.long)\n",
    "    tokenized_evaluation_data = torch.tensor(tokenize_func(eval_text, string_to_int), dtype=torch.long)\n",
    "    \n",
    "    # If a maximum number of tokens is defined, truncate the training dataset\n",
    "    if max_train_tokens is not None:\n",
    "        tokenized_training_data = tokenized_training_data[:max_train_tokens]\n",
    "    \n",
    "    return tokenized_training_data, tokenized_evaluation_data\n",
    "\n",
    "# --- Extract sub-batch ---\n",
    "def get_batch(data_partition_name, training_data, evaluation_data, context_length, batch_size, device):\n",
    "    data = training_data if data_partition_name == 'train' else evaluation_data\n",
    "    max_offset = len(data) - context_length - 1\n",
    "    random_start_offsets = torch.randint(max_offset, (batch_size,))\n",
    "    input_tokens = torch.stack([data[offset:offset+context_length] for offset in random_start_offsets])\n",
    "    solution_tokens = torch.stack([data[offset+1:offset+1+context_length] for offset in random_start_offsets])\n",
    "    return input_tokens.to(device), solution_tokens.to(device)\n",
    "\n",
    "# --- Loss evaluation functions ---\n",
    "@torch.no_grad()\n",
    "def calculate_mean_losses(model, training_data, evaluation_data, context_length, batch_size, eval_iteration_count, device, get_batch_func):\n",
    "    mean_losses = {}\n",
    "    model.eval()\n",
    "    for data_partition_name in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iteration_count)\n",
    "        for eval_iteration_number in range(eval_iteration_count):\n",
    "            inputs, solutions = get_batch_func(data_partition_name, training_data, evaluation_data, context_length, batch_size, device)\n",
    "            _, loss = model(inputs, solutions)\n",
    "            losses[eval_iteration_number] = loss.item()\n",
    "        mean_losses[data_partition_name] = losses.mean()\n",
    "    model.train()\n",
    "    return mean_losses\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_short_mean_losses(model, training_data, evaluation_data, context_length, batch_size, short_eval_iters, device, get_batch_func):\n",
    "    mean_losses = {}\n",
    "    model.eval()\n",
    "    for data_partition_name in ['train', 'val']:\n",
    "        losses = torch.zeros(short_eval_iters)\n",
    "        for eval_iteration_number in range(short_eval_iters):\n",
    "            inputs, solutions = get_batch_func(data_partition_name, training_data, evaluation_data, context_length, batch_size, device)\n",
    "            _, loss = model(inputs, solutions)\n",
    "            losses[eval_iteration_number] = loss.item()\n",
    "        mean_losses[data_partition_name] = losses.mean()\n",
    "    model.train()\n",
    "    return mean_losses\n",
    "\n",
    "# --- Model classes ---\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, head_size, embedding_dimension_count, context_length, dropout):\n",
    "        super().__init__()\n",
    "        self.keys = nn.Linear(embedding_dimension_count, head_size, bias=False)\n",
    "        self.queries = nn.Linear(embedding_dimension_count, head_size, bias=False)\n",
    "        self.values = nn.Linear(embedding_dimension_count, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "        self.dropouts = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, current_token_contexts):\n",
    "        batch_size, token_count, channel_count = current_token_contexts.shape\n",
    "        keys = self.keys(current_token_contexts)\n",
    "        queries = self.queries(current_token_contexts)\n",
    "        attention_scores = queries @ keys.transpose(-2, -1) * keys.shape[-1] ** -0.5\n",
    "        causal_attention_scores = attention_scores.masked_fill(self.tril[:token_count, :token_count] == 0, float('-inf'))\n",
    "        probabilistic_causal_attention = F.softmax(causal_attention_scores, dim=-1)\n",
    "        probabilistic_causal_attention = self.dropouts(probabilistic_causal_attention)\n",
    "        values = self.values(current_token_contexts)\n",
    "        shared_informations = probabilistic_causal_attention @ values\n",
    "        return shared_informations\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_count, head_size, embedding_dimension_count, context_length, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size, embedding_dimension_count, context_length, dropout) for _ in range(head_count)])\n",
    "        self.projection = nn.Linear(embedding_dimension_count, embedding_dimension_count)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        self_attended_tokens = torch.cat([head(input_tokens) for head in self.heads], dim=-1)\n",
    "        projection = self.projection(self_attended_tokens)\n",
    "        projection = self.dropout(projection)\n",
    "        return projection\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dimension_count, dropout):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embedding_dimension_count, 4 * embedding_dimension_count),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embedding_dimension_count, embedding_dimension_count),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_tokens):\n",
    "        return self.network(input_tokens)\n",
    "\n",
    "class AttentionThinkingBlock(nn.Module):\n",
    "    def __init__(self, embedding_dimension_count, head_count, context_length, dropout):\n",
    "        super().__init__()\n",
    "        head_size = embedding_dimension_count // head_count\n",
    "        self.attention_network = MultiHeadAttention(head_count, head_size, embedding_dimension_count, context_length, dropout)\n",
    "        self.feed_forward_network = FeedForwardNetwork(embedding_dimension_count, dropout)\n",
    "        self.attention_layer_normalization = nn.LayerNorm(embedding_dimension_count)\n",
    "        self.feed_forward_layer_normalization = nn.LayerNorm(embedding_dimension_count)\n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        normalized_input_tokens = self.attention_layer_normalization(input_tokens)\n",
    "        attended_tokens = input_tokens + self.attention_network(normalized_input_tokens)\n",
    "        normalized_attended_tokens = self.feed_forward_layer_normalization(attended_tokens)\n",
    "        thought_attended_tokens = attended_tokens + self.feed_forward_network(normalized_attended_tokens)\n",
    "        return thought_attended_tokens\n",
    "\n",
    "class GptOne(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dimension_count, context_length, dropout, head_count, layer_count, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, embedding_dimension_count)\n",
    "        self.position_embedding_table = nn.Embedding(context_length, embedding_dimension_count)\n",
    "        self.attention_thinking_blocks = nn.Sequential(\n",
    "            *[AttentionThinkingBlock(embedding_dimension_count, head_count, context_length, dropout) for _ in range(layer_count)]\n",
    "        )\n",
    "        self.final_layer_normalization = nn.LayerNorm(embedding_dimension_count)\n",
    "        self.language_modeling_head = nn.Linear(embedding_dimension_count, vocabulary_size)\n",
    "\n",
    "    def forward(self, input_tokens, solution_tokens=None):\n",
    "        batch_size, token_count = input_tokens.shape\n",
    "        token_embeddings = self.token_embedding_table(input_tokens)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(token_count, device=self.device))\n",
    "        spatial_meaning_embedding = token_embeddings + position_embeddings\n",
    "        spatial_meaning_embedding = self.attention_thinking_blocks(spatial_meaning_embedding)\n",
    "        normalized_thought_embedding = self.final_layer_normalization(spatial_meaning_embedding)\n",
    "        logits = self.language_modeling_head(normalized_thought_embedding)\n",
    "        if solution_tokens is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, token_count, channel_size = logits.shape\n",
    "            logits = logits.view(batch_size * token_count, channel_size)\n",
    "            solution_tokens = solution_tokens.view(batch_size * token_count)\n",
    "            loss = F.cross_entropy(logits, solution_tokens)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, input_tokens, max_new_token_number, context_length):\n",
    "        for _ in range(max_new_token_number):\n",
    "            context_tokens = input_tokens[:, -context_length:]\n",
    "            logits, _ = self(context_tokens)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_tokens = torch.cat((input_tokens, next_token), dim=1)\n",
    "        return input_tokens\n",
    "\n",
    "# --- Checkpoint saving and loading ---\n",
    "def save_checkpoint(model, loss, hyperparams, checkpoint_dir=\"checkpoints\", base_name=\"gpt_wiki_bigram_two\"):\n",
    "    \"\"\"\n",
    "    Save the checkpoint with the hyperparameters in the name\n",
    "    that intrinsically define the model.\n",
    "    \n",
    "    hyperparams: dictionary containing for example:\n",
    "        {\n",
    "            'head_count': 12,\n",
    "            'layer_count': 2,\n",
    "            'embedding_dimension_count': 576,\n",
    "            'context_length': 364,\n",
    "            'dropout': 0.10\n",
    "        }\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    # Construire une chaîne représentant les hyperparamètres\n",
    "    hp_str = f\"heads{hyperparams['head_count']}_layers{hyperparams['layer_count']}_emb{hyperparams['embedding_dimension_count']}_ctx{hyperparams['context_length']}_drop{hyperparams['dropout']}\"\n",
    "    \n",
    "    existing = [f for f in os.listdir(checkpoint_dir) if f.startswith(base_name) and f.endswith(\".pt\")]\n",
    "    max_index = 0\n",
    "    for fname in existing:\n",
    "        try:\n",
    "            idx = int(fname.split('_')[-2])\n",
    "            max_index = max(max_index, idx)\n",
    "        except Exception:\n",
    "            continue\n",
    "    new_index = max_index + 1\n",
    "    loss_int = int(loss * 10000)\n",
    "    checkpoint_name = f\"{base_name}_{hp_str}_{new_index}_loss{loss_int}.pt\"\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(\"Checkpoint sauvegardé :\", checkpoint_path)\n",
    "\n",
    "def load_checkpoint(model, checkpoint_path, device):\n",
    "    state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(\"Checkpoint chargé depuis :\", checkpoint_path)\n",
    "\n",
    "# --- Fonctions de génération de texte ---\n",
    "def generate_text(model, detokenize_func, int_to_string, max_new_token_number, context_length, device):\n",
    "    starting_context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    generated_tokens = model.generate(starting_context, max_new_token_number, context_length)\n",
    "    generated_text = detokenize_func(generated_tokens[0].tolist(), int_to_string)\n",
    "    return generated_text\n",
    "\n",
    "def generate_and_print_text(model, context_length, detokenize_func, int_to_string, max_new_token_number, tokens_per_print, starting_context):\n",
    "    # Affiche le contexte initial\n",
    "    print(detokenize_func(starting_context[0].tolist(), int_to_string), end='', flush=True)\n",
    "    generated_tokens = starting_context\n",
    "    for _ in range(max_new_token_number // tokens_per_print):\n",
    "        # Génère tokens_per_print tokens de plus\n",
    "        generated_tokens = model.generate(generated_tokens, tokens_per_print, context_length)\n",
    "        # Extraction des tokens générés lors de cette itération\n",
    "        new_token_indices = generated_tokens[0].tolist()[-tokens_per_print:]\n",
    "        new_text = ''.join([int_to_string[t] for t in new_token_indices])\n",
    "        print(new_text, end='', flush=True)\n",
    "\n",
    "\n",
    "def generate_and_print_text(model, context_length, detokenize_func, int_to_string,\n",
    "                            max_new_token_number, tokens_per_print, starting_context, return_text=False):\n",
    "    \"\"\"\n",
    "    Génère et affiche le texte par incréments de tokens.\n",
    "    Si return_text est True, renvoie le texte généré en plus de l'affichage.\n",
    "    \"\"\"\n",
    "    # Initialisation\n",
    "    text_generated = detokenize_func(starting_context[0].tolist(), int_to_string)\n",
    "    print(text_generated, end='', flush=True)\n",
    "    generated_tokens = starting_context\n",
    "    \n",
    "    steps = max_new_token_number // tokens_per_print\n",
    "    for _ in range(steps):\n",
    "        generated_tokens = model.generate(generated_tokens, tokens_per_print, context_length)\n",
    "        full_text = detokenize_func(generated_tokens[0].tolist(), int_to_string)\n",
    "        # Extraction sur la base des tokens générés et non des caractères\n",
    "        new_token_indices = generated_tokens[0].tolist()[-tokens_per_print:]\n",
    "        new_text = ''.join([int_to_string[t] for t in new_token_indices])\n",
    "        print(new_text, end='', flush=True)\n",
    "        text_generated += new_text\n",
    "        \n",
    "    if return_text:\n",
    "        return text_generated\n",
    "\n",
    "def generate_print_and_save_text(model, context_length, detokenize_func, int_to_string,\n",
    "                                 max_new_token_number, tokens_per_print, starting_context, file_name):\n",
    "    \"\"\"\n",
    "    Appelle generate_and_print_text pour générer et afficher le texte,\n",
    "    puis sauvegarde l'intégralité dans un fichier.\n",
    "    \"\"\"\n",
    "    final_text = generate_and_print_text(model, context_length, detokenize_func, int_to_string,\n",
    "                                           max_new_token_number, tokens_per_print, starting_context,\n",
    "                                           return_text=True)\n",
    "    time.sleep(10)\n",
    "    save_str_to_file(final_text, file_name)\n",
    "    print(f\"\\nTexte intégral sauvegardé dans '{file_name}'.\")\n",
    "\n",
    "\n",
    "def inspect_characters(text):\n",
    "    for idx, c in enumerate(text):\n",
    "        code_point = ord(c)\n",
    "        name = unicodedata.name(c, \"UNKNOWN\")\n",
    "        print(f\"{idx:3d} | {repr(c)} | U+{code_point:04X} | {name}\")\n",
    "# Write the first 1000 re-detokenized tokens to a text file\n",
    "def write_first_1000_tokens_to_file(tokenized_data, file_name, detokenize_func, int_to_string):\n",
    "    first_1000 = tokenized_data.tolist()[:1000]\n",
    "    detokenized_text = detokenize_func(first_1000, int_to_string)\n",
    "    save_str_to_file(detokenized_text, file_name)\n",
    "# --- Boucle d'entraînement ---\n",
    "def perform_long_evaluation(step, best_val_loss, no_improvement_count, max_no_improvement,\n",
    "                            model, training_data, evaluation_data, context_length, batch_size,\n",
    "                            eval_iteration_count, device, get_batch_func, hyperparams):\n",
    "    print(f\"Evaluating losses at step {step}...\")\n",
    "    losses = calculate_mean_losses(model, training_data, evaluation_data, context_length, batch_size, eval_iteration_count, device, get_batch_func)\n",
    "    print(f\"step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    print(f\"Current min_loss: {best_val_loss:.4f}\")\n",
    "    if losses['val'] < best_val_loss:\n",
    "        best_val_loss = losses['val']\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "        if no_improvement_count >= max_no_improvement:\n",
    "            print(f\"Validation loss did not improve for {max_no_improvement} consecutive evaluations. Stopping training.\")\n",
    "            save_checkpoint(model, losses['val'], hyperparams)\n",
    "            return True, best_val_loss, no_improvement_count\n",
    "    return False, best_val_loss, no_improvement_count\n",
    "\n",
    "def train(model, training_data, evaluation_data, context_length, batch_size, maximum_training_steps,\n",
    "          evaluation_interval, short_eval_interval, checkpoint_interval, generate_interval,\n",
    "          time_estimation_interval, eval_iteration_count, short_eval_iters, learning_rate, device,\n",
    "          max_new_token_number_preview, generate_and_print_text_func, get_batch_func,\n",
    "          calculate_mean_losses_func, calculate_short_mean_losses_func, save_checkpoint_func,\n",
    "          tokenize_func, string_to_int, detokenize_func, int_to_string, hyperparams):\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    print('THE MODEL HAS STARTED TRAINING')\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_short_eval_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    max_no_improvement = 500\n",
    "    short_no_improvement_count = 0\n",
    "    max_short_no_improvement = 500\n",
    "    starting_timer = time.time()\n",
    "    \n",
    "    for step in range(maximum_training_steps):\n",
    "        if step % evaluation_interval == 0 or step == maximum_training_steps - 1:\n",
    "            stop_training, best_val_loss, no_improvement_count = perform_long_evaluation(\n",
    "                step, best_val_loss, no_improvement_count, max_no_improvement,\n",
    "                model, training_data, evaluation_data, context_length, batch_size,\n",
    "                eval_iteration_count, device, get_batch_func, hyperparams)\n",
    "            if stop_training:\n",
    "                break\n",
    "        \n",
    "        if step % short_eval_interval == 0:\n",
    "            print(f\"Performing short evaluation at step {step}...\")\n",
    "            short_losses = calculate_short_mean_losses_func(\n",
    "                model, training_data, evaluation_data, context_length, batch_size, short_eval_iters, device, get_batch_func)\n",
    "            print(f\"step {step}: short train loss {short_losses['train']:.4f}, short val loss {short_losses['val']:.4f}\")\n",
    "            print(f\"Current min_short_loss: {best_short_eval_loss:.4f}\")\n",
    "            if short_losses['val'] < best_short_eval_loss:\n",
    "                best_short_eval_loss = short_losses['val']\n",
    "                short_no_improvement_count = 0\n",
    "            else:\n",
    "                short_no_improvement_count += 1\n",
    "                if short_no_improvement_count >= max_short_no_improvement:\n",
    "                    stop_training, best_val_loss, no_improvement_count = perform_long_evaluation(\n",
    "                        step, best_val_loss, no_improvement_count, max_no_improvement,\n",
    "                        model, training_data, evaluation_data, context_length, batch_size,\n",
    "                        eval_iteration_count, device, get_batch_func, hyperparams)\n",
    "                    if stop_training:\n",
    "                        break\n",
    "        \n",
    "        if step % checkpoint_interval == 0 or step == maximum_training_steps - 1:\n",
    "            print(f\"Saving checkpoint at step {step}...\")\n",
    "            save_checkpoint_func(model, best_val_loss, hyperparams)\n",
    "        \n",
    "        if step % generate_interval == 0 or step == maximum_training_steps - 1:\n",
    "            print(f\"Generating text at step {step}...\")\n",
    "            starting_context = torch.tensor(tokenize_func(\"John Lennon est \", string_to_int), dtype=torch.long, device=device).unsqueeze(0)\n",
    "            generate_and_print_text_func(model, context_length, detokenize_func, int_to_string, max_new_token_number_preview, 1, starting_context)\n",
    "        \n",
    "        if step % time_estimation_interval == 0 or step == maximum_training_steps - 1:\n",
    "            estimate_time(maximum_training_steps, starting_timer, step)\n",
    "        \n",
    "        random_input_tokens, solution_tokens = get_batch_func('train', training_data, evaluation_data, context_length, batch_size, device)\n",
    "        logits, loss = model(random_input_tokens, solution_tokens)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print('Training has finished :)')\n",
    "    print(datetime.now())\n",
    "\n",
    "def estimate_time(maximum_training_steps, starting_timer, step):\n",
    "    print(f\"Estimating remaining time at step {step}...\")\n",
    "    current_time = time.time()\n",
    "    current_training_duration = current_time - starting_timer\n",
    "    minutes_by_step = current_training_duration / (step + 1) / 60\n",
    "    remaining_steps = maximum_training_steps - step\n",
    "    remaining_minutes = remaining_steps * minutes_by_step\n",
    "    predicted_end_time = datetime.now() + timedelta(minutes=remaining_minutes)\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Step: {step}/{maximum_training_steps}\")\n",
    "    print(f\"Elapsed Time: {current_training_duration / 60:.2f} minutes\")\n",
    "    print(f\"Remaining Time: {remaining_minutes:.2f} minutes\")\n",
    "    print(f\"Predicted End Time: {predicted_end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# --- Programme principal ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Définition des hyperparamètres\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    tokenization_iteration = 2000\n",
    "    batch_size = 64 \n",
    "    context_length = 500\n",
    "    maximum_training_steps = 25000\n",
    "    learning_rate = 2e-3\n",
    "    head_count = 6\n",
    "    layer_count = 4\n",
    "    dropout = 0.10\n",
    "    embedding_dimension_count = 360 \n",
    "    evaluation_interval = 800\n",
    "    eval_iteration_count = 30\n",
    "    short_eval_interval = 400\n",
    "    short_eval_iters = 5\n",
    "    max_new_token_number = 1000\n",
    "    max_new_token_number_preview = 100\n",
    "    model_file_name = \"gpt_wiki_octogram_one_mini\"\n",
    "    generate_interval = 800\n",
    "    checkpoint_interval = 5000\n",
    "    time_estimation_interval = 200\n",
    "    should_train = True\n",
    "    should_load = False\n",
    "    model_to_load = \"checkpoints/gpt_wiki_bigram_two_heads6_layers4_emb360_ctx500_drop0.1_19_loss21833\"\n",
    "    use_tokenizer = False\n",
    "    tokenizer_to_load =\"\"\n",
    "    # Chargement des données\n",
    "    training_text, eval_text = load_data('../wiki.train.tokens', '../wiki.test.tokens')\n",
    "    \n",
    "    if use_tokenizer:\n",
    "        string_to_int, int_to_string = load_tokenizer(tokenizer_to_load)\n",
    "        vocabulary_size = len(string_to_int)\n",
    "    else:\n",
    "        vocabulary_size, string_to_int, int_to_string, tokenizer_path = create_vocabularies_V2(training_text, tokenization_iteration=tokenization_iteration)\n",
    "\n",
    "\n",
    "    # Préparation des tenseurs de données\n",
    "    tokenized_training_data, tokenized_evaluation_data = prepare_tokenized_data(training_text, eval_text, tokenize, string_to_int)\n",
    "    print(\"training set size chars :\")\n",
    "    char_count = len(set(tokenized_training_data))\n",
    "    print(char_count)\n",
    "    token_count = len(tokenized_training_data)\n",
    "    print(\"training set size tokenized :\")\n",
    "    print(token_count)\n",
    "    print(\"compression ratio:\")\n",
    "    print(char_count/token_count)\n",
    "    time.sleep(3)\n",
    "    print(\"tokens by iteration :\")\n",
    "    print(len(tokenized_training_data) / (maximum_training_steps * batch_size))\n",
    "    # Sauvegarde d'extraits\n",
    "    write_first_1000_tokens_to_file(tokenized_training_data, 'first_1000_tokens.txt', detokenize, int_to_string)\n",
    "    write_first_2000_chars_to_file(training_text, 'first_2000_chars.txt')\n",
    "\n",
    "    # Création du modèle\n",
    "    model = GptOne(vocabulary_size, embedding_dimension_count, context_length, dropout, head_count, layer_count, device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Définir les hyperparamètres pour la sauvegarde\n",
    "    hyperparams = {\n",
    "        'head_count': head_count,\n",
    "        'layer_count': layer_count,\n",
    "        'embedding_dimension_count': embedding_dimension_count,\n",
    "        'context_length': context_length,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    if(should_load):\n",
    "        model = load_checkpoint(model=model,checkpoint_path=model_to_load,device=device)\n",
    "    # Entraînement\n",
    "    if(should_train):\n",
    "        train(model,\n",
    "            tokenized_training_data,\n",
    "            tokenized_evaluation_data,\n",
    "            context_length,\n",
    "            batch_size,\n",
    "            maximum_training_steps,\n",
    "            evaluation_interval,\n",
    "            short_eval_interval,\n",
    "            checkpoint_interval,\n",
    "            generate_interval,\n",
    "            time_estimation_interval,\n",
    "            eval_iteration_count,\n",
    "            short_eval_iters,\n",
    "            learning_rate,\n",
    "            device,\n",
    "            max_new_token_number_preview,\n",
    "            generate_and_print_text,\n",
    "            get_batch,\n",
    "            calculate_mean_losses,\n",
    "            calculate_short_mean_losses,\n",
    "            save_checkpoint,\n",
    "            tokenize,\n",
    "            string_to_int,\n",
    "            detokenize,\n",
    "            int_to_string,\n",
    "            hyperparams)\n",
    "\n",
    "    # Génération finale et sauvegarde\n",
    "    starting_context = torch.tensor(tokenize(\"En 1998, la coupe du monde a été gagnée par\", string_to_int), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    generate_print_and_save_text(model, context_length, detokenize, int_to_string, max_new_token_number, 1, starting_context, 'generated_text.txt')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
